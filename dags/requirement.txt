Choose a dataset that has an outcome (predictive) variable.
Split that into train and test.
Define a metric to evaluate a machine learning model.
Build a pipeline using Airflow or MLflow or your platform pipeline to train a machine learning model using the train dataset (use AutoML to refine the category of algorithms).
Deploy the model for inference.
Set up model monitoring (if there is a monitoring dashboard show that).
Use the test data with the deployed model and validate the results (metric) and model monitoring.
Change atleast 2 feature values of the test dataset (you can put in random values or swap 2 features).
Use the "changed" test data with the deployed model and validate the results (metric) and verify observation with model monitoring.
Presentation in class that covers the above points with a video demo of #7 and #9.

The presentation PPT should include

EDA of your chosen dataset
Metric to evaluate the machine learning model
Pipeline you used to train the machine learning model
AutoML results and the chosen algorithm
Model monitoring
The "change" in the test dataset
Github link that contains the project code
Clearly outline which member did which part of the project.
