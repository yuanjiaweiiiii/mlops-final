# -*- coding: utf-8 -*-
"""H2O_automl_evidently_deploy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13UzEWNQrRGPohPpJeHsHuoxGcdZ2MN9I
"""

!pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o

"""### Step 1: Install and Import H2O"""

import h2o
from h2o.automl import H2OAutoML
h2o.init()

"""### Step 2: Load Train/Test Data into H2O"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

df = pd.read_csv('healthcare-dataset-stroke-data.csv')
#  Train-Test Split
# ============================
# Drop ID column if it exists
if 'id' in df.columns:
    df = df.drop(columns=['id'])

target_col = 'stroke'
X = df.drop(columns=[target_col])
y = df[target_col]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("\nTrain set shape:", X_train.shape)
print("Test set shape:", X_test.shape)



# Join X and y so H2OFrame has the target in the same table
train_pd = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)
test_pd  = pd.concat([X_test.reset_index(drop=True),  y_test.reset_index(drop=True)],  axis=1)

train_h2o = h2o.H2OFrame(train_pd)
test_h2o  = h2o.H2OFrame(test_pd)

# Cast columns: make target categorical; make string/object columns categorical too
y = target_col
x = [c for c in train_h2o.columns if c != y]

# If this is classification, ensure factor
train_h2o[y] = train_h2o[y].asfactor()
test_h2o[y]  = test_h2o[y].asfactor()

# Optional: make object/string columns categorical
for c in x:
    if train_pd[c].dtype == "object":
        train_h2o[c] = train_h2o[c].asfactor()
        test_h2o[c]  = test_h2o[c].asfactor()

"""### Step 3: Run AutoML"""

aml = H2OAutoML(
    max_runtime_secs=600,
    seed=42,
    nfolds=5,
    balance_classes=True,
    sort_metric="AUCPR",
    stopping_metric="AUCPR",
    verbosity="info"
)
aml.train(x=x, y=y, training_frame=train_h2o)

"""### Step 4: Leaderboard + pick the best model"""

lb = aml.leaderboard
lb.head(rows=lb.nrows)
leader = aml.leader
leader.model_id

"""### Step 5: Evaluate on test set"""

perf = leader.model_performance(test_h2o)
print(perf)

# Pull common metrics
test_auc    = perf.auc()
test_logloss= perf.logloss()
test_f1_opt = perf.F1()  # best F1 across thresholds
# Find the row with maximum F1
best_f1_row = max(test_f1_opt, key=lambda x: x[1])  # x[1] = F1 value

thresh = best_f1_row[0]
test_f1_opt = best_f1_row[1]
print({"AUC": test_auc, "LogLoss": test_logloss, "Best F1": test_f1_opt, "Threshold": thresh})

# Confusion matrix at F1-optimal threshold
cm = perf.confusion_matrix(metrics="f1", thresholds=[thresh])
cm

"""### Step 6: Save the model for deployment"""

model_dir = "models_h2o"
native_path = h2o.save_model(leader, path=model_dir, force=True)
mojo_path   = leader.download_mojo(path=model_dir, get_genmodel_jar=True)
print(native_path, mojo_path)

"""### Step 7: Get predictions on test set"""

pred = leader.predict(test_h2o)  # columns: p0, p1, predict
pred.head()
# Optionally join predictions back to original rows:
pred_with_labels = test_h2o.cbind(pred)
pred_with_labels.head()

"""### Step 8: Change at least two features and re-validate"""

changed = test_h2o[:]
num_cols = [c for c in x if test_pd[c].dtype != "object"]

# pick two numeric features to perturb
feat_a, feat_b = ("bmi", "avg_glucose_level") if {"bmi","avg_glucose_level"}.issubset(set(num_cols)) else (num_cols[0], num_cols[1])

# add small noise / shift  (tune scale as you like)
changed[feat_a] = changed[feat_a] * 1.10           # +10%
changed[feat_b] = changed[feat_b] + 5              # +5 absolute units

perf_changed = leader.model_performance(changed)

# Find the row with maximum F1
test_f1_opt = perf_changed.F1()
best_f1_row = max(test_f1_opt, key=lambda x: x[1])  # x[1] = F1 value
test_f1_opt = best_f1_row[1]
print("Changed data metrics:", {
    "AUC": perf_changed.auc(),
    "LogLoss": perf_changed.logloss(),
    "Best F1": test_f1_opt
})

"""### Step 9: Quick monitoring hooks you can show"""

import pandas as pd, datetime as dt

def summarize_perf(perf_obj, tag):
    test_f1_opt = perf_obj.F1()
    best_f1_row = max(test_f1_opt, key=lambda x: x[1])
    test_f1_opt = best_f1_row[1]
    return {
        "when": dt.datetime.utcnow().isoformat(),
        "tag": tag,
        "auc": perf_obj.auc(),
        "logloss": perf_obj.logloss(),
        "f1_best": test_f1_opt
    }

logs = []
logs.append(summarize_perf(perf, "baseline_test"))
logs.append(summarize_perf(perf_changed, "changed_numeric"))  # if you ran Option A
pd.DataFrame(logs).to_csv("automl_scoring_log.csv", index=False)

log = pd.read_csv("automl_scoring_log.csv")
log

"""# Evidently Drift Analysis"""

!pip install evidently

from evidently import Dataset
from evidently import DataDefinition
from evidently import Report
from evidently.presets import DataDriftPreset, DataSummaryPreset

from evidently.ui.workspace import CloudWorkspace

# Convert H2O frames to pandas for Evidently
print("Converting H2O frames to pandas")
stroke_ref = test_h2o.as_data_frame()     # Reference data (baseline test set)
stroke_prod = changed.as_data_frame()     # Production data (modified test set)

print(f"Reference data shape: {stroke_ref.shape}")
print(f"Production data shape: {stroke_prod.shape}")

# Define Evidently schema
schema = DataDefinition(
    numerical_columns=["age", "avg_glucose_level", "bmi"],
    categorical_columns=["gender", "hypertension", "heart_disease", "ever_married",
                       "work_type", "Residence_type", "smoking_status", "stroke"],
)

# Create Evidently datasets
eval_data_1 = Dataset.from_pandas(
    pd.DataFrame(stroke_prod),  # Current/production data (changed features)
    data_definition=schema
)

eval_data_2 = Dataset.from_pandas(
    pd.DataFrame(stroke_ref),   # Reference data (original test set)
    data_definition=schema
)

# Create and run Evidently report
print("Running Evidently drift analysis...")
report = Report([
    DataDriftPreset()
])

# Run the analysis
my_eval = report.run(eval_data_1, eval_data_2)

print("Evidently Analysis")

# Save HTML report
timestamp = dt.datetime.now().strftime('%Y%m%d_%H%M%S')
html_filename = f'stroke_drift_report_{timestamp}.html'
my_eval.save_html(html_filename)
print(f"Drift report saved: {html_filename}")

my_eval

# monitoring summary
print("Enhanced Monitoring Summary:")
print("=" * 30)

# Get existing metrics from the log
existing_log = pd.read_csv("automl_scoring_log.csv")
print("Performance Tracking:")
print(existing_log)

# Calculate performance changes
baseline_auc = existing_log[existing_log['tag'] == 'baseline_test']['auc'].iloc[0]
changed_auc = existing_log[existing_log['tag'] == 'changed_numeric']['auc'].iloc[0]
auc_change = changed_auc - baseline_auc

baseline_f1 = existing_log[existing_log['tag'] == 'baseline_test']['f1_best'].iloc[0]
changed_f1 = existing_log[existing_log['tag'] == 'changed_numeric']['f1_best'].iloc[0]
f1_change = changed_f1 - baseline_f1

print(f"\nPerformance Impact Analysis:")
print(f"   Baseline AUC: {baseline_auc:.4f}")
print(f"   Modified AUC: {changed_auc:.4f}")
print(f"   AUC Change: {auc_change:+.4f} ({auc_change/baseline_auc*100:+.2f}%)")

print(f"\nBaseline F1: {baseline_f1:.4f}")
print(f"   Modified F1: {changed_f1:.4f}")
print(f"   F1 Change: {f1_change:+.4f} ({f1_change/baseline_f1*100:+.2f}%)")

print(f"\nFiles Generated:")
print(f"   Original H2O Model: {native_path}")
print(f"   MOJO Model: {mojo_path}")
print(f"   Performance Log: automl_scoring_log.csv")
print(f"   Drift Report: {html_filename}")



print("Model Deployment for Inference")
print("=" * 50)

deployed_model = h2o.load_model(native_path)

# Get predictions on first 5 test samples
sample_size = 5
sample_patients = test_h2o[0:sample_size]
predictions = deployed_model.predict(sample_patients)

print(f"Inference completed for {sample_size} patients")
print("Prediction results:")
pred_df = predictions.as_data_frame()
print(pred_df)

# Show risk assessment
print("\nStroke Risk Assessment:")
for i in range(sample_size):
    patient_id = i + 1
    pred_prob = float(pred_df.iloc[i]['p1'])  # Probability of stroke
    pred_class = int(pred_df.iloc[i]['predict'])

    if pred_prob > 0.5:
        risk_level = "HIGH"
    elif pred_prob > 0.2:
        risk_level = "MEDIUM"
    else:
        risk_level = "LOW"

    print(f"Patient {patient_id}: {pred_prob:.1%} stroke risk ({risk_level}) | Prediction: {pred_class}")

# Performance on full test set (deployment validation)
print("\nDeployment Validation - Full Test Set Performance:")
test_predictions = deployed_model.predict(test_h2o)
test_performance = deployed_model.model_performance(test_h2o)

print(f"Deployed Model AUC: {test_performance.auc():.4f}")
print(f"Deployed Model Log Loss: {test_performance.logloss():.4f}")

# Verify deployment matches original results
print(f"Original Model AUC: {perf.auc():.4f}")
print(f"Deployment Verification: {'PASS' if abs(test_performance.auc() - perf.auc()) < 0.001 else 'FAIL'}")

# Create simple inference function
print("\nCreating production inference function...")

def predict_stroke_risk(patient_data_h2o):
    """
    Production inference function
    """
    try:
        predictions = deployed_model.predict(patient_data_h2o)
        return predictions
    except Exception as e:
        print(f"Inference error: {e}")
        return None

# Test the inference function
print("Testing inference function...")
test_result = predict_stroke_risk(test_h2o[0:3])
if test_result is not None:
    print("Inference function working correctly")
    single_pred = test_result.as_data_frame()
    print(f"Sample prediction: {float(single_pred.iloc[0]['p1']):.1%} stroke risk")
else:
    print("Inference function failed")

# Step 6: Model deployment summary
print("\n" + "=" * 30)
print("DEPLOYMENT SUMMARY")
print("=" * 30)
print(f"Model ID: {deployed_model.model_id}")
print(f"Model Type: {type(deployed_model).__name__}")
print(f"Inference Capability: Verified")
print(f"Performance Match: {test_performance.auc():.4f} AUC")
print(f"Production Ready")

print(f"\n Ready for production deployment")
print(f"   Model can now serve predictions via API endpoint")

# Check for stroke predictions (class 1) in test set
print("Prediction Distribution")
print("=" * 50)

# Get full test set predictions
full_predictions = deployed_model.predict(test_h2o)
pred_df = full_predictions.as_data_frame()

# Analyze prediction distribution
print("Prediction Summary:")
print(f"Total predictions: {len(pred_df)}")
print(f"Class 0 (No Stroke): {sum(pred_df['predict'] == 0)}")
print(f"Class 1 (Stroke): {sum(pred_df['predict'] == 1)}")
print(f"Percentage predicting stroke: {sum(pred_df['predict'] == 1)/len(pred_df)*100:.2f}%")

# Show probability distribution
print(f"\nStroke Probability Distribution:")
print(f"Min probability: {pred_df['p1'].min():.4f}")
print(f"Max probability: {pred_df['p1'].max():.4f}")
print(f"Mean probability: {pred_df['p1'].mean():.4f}")
print(f"Median probability: {pred_df['p1'].median():.4f}")

# Find high-risk patients
high_risk = pred_df[pred_df['p1'] > 0.1]  # >10% stroke risk
print(f"\nPatients with >10% stroke risk: {len(high_risk)}")

very_high_risk = pred_df[pred_df['p1'] > 0.3]  # >30% stroke risk
print(f"Patients with >30% stroke risk: {len(very_high_risk)}")

# Show the highest risk patients
print(f"\nTop 10 Highest Risk Patients:")
top_risk = pred_df.nlargest(10, 'p1')
for i, (idx, row) in enumerate(top_risk.iterrows()):
    prob = row['p1']
    pred = int(row['predict'])
    print(f"Patient {idx}: {prob:.1%} stroke risk | Prediction: {pred}")

# Check actual vs predicted distribution
actual_test = test_h2o['stroke'].as_data_frame()
print(f"\nActual vs Predicted Distribution:")
print(f"Actual strokes in test set: {sum(actual_test['stroke'])}")
print(f"Predicted strokes: {sum(pred_df['predict'])}")
print(f"Actual stroke rate: {sum(actual_test['stroke'])/len(actual_test)*100:.2f}%")

!pip install -q mlflow

!pip install pyngrok